<p align="center">
  <img src="assets/GH_OFL_logo.png" width="300">
</p>

# GH-OFL â€” The Gaussian Head Family  
### One-Shot Federated Learning from Client Global Statistics  
**ICLR 2026 â€” Official Camera-Ready Implementation**

---

## ğŸ“Œ Overview

**GH-OFL (Gaussian-Head One-Shot Federated Learning)** is a statistics-driven federated learning framework where clients transmit global feature statistics only, enabling server-side closed-form and lightweight trainable classifiers â€” without gradient exchange, multi-round optimization, or raw data transmission.

This repository contains the refined camera-ready implementation associated with:

Turazza, Picone, Mamei.  
*The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics.*  
ICLR 2026.

<p align="center">ğŸ“„ <a href="https://arxiv.org/abs/2602.01186"><b>Read the paper on arXiv (2602.01186)</b></a></p>

---

# ğŸ§  Core Idea

Instead of sharing gradients or local model weights, each client computes and transmits:

- Per-class feature sums  
- Diagonal second moments  
- Full covariance matrices (optional, enables QDA)  
- Global second-order statistics  

The server reconstructs Gaussian decision heads analytically and optionally refines them in a Fisher subspace.

âœ” One-shot communication  
âœ” Statistics-only federation  
âœ” No gradient aggregation  
âœ” No raw data exchange  
âœ” No iterative client training  

---

# ğŸ“Š Implemented Heads

## Closed-Form (x-space)

- **GH-NBdiag** â€” Diagonal Gaussian classifier  
- **GH-LDA** â€” Pooled covariance (shrinkage = 0.05)  
- **GH-QDAfull** â€” Full class covariance (GPU optimized)  

## Trainable (Fisher space)

- **FisherMix** â€” Cosine classifier on Fisher projections  
- **Proto-Hyper** â€” Low-rank residual adapter with knowledge distillation  

Proto-Hyper formulation:

Student(z_f) = Standardize(LDA_f(z_f)) + LowRankResidual(z_f)  
Teacher = Î» Â· QDA_f + (1 âˆ’ Î») Â· LDA_f  
Loss = Knowledge Distillation (KL) + Cross-Entropy  

Inference uses the student model only.

---

# ğŸ“ Repository Structure

GH-OFL/  
â”‚  
â”œâ”€â”€ client_cifar10.py  
â”œâ”€â”€ server_cifar10.py  
â”œâ”€â”€ client_cifar100.py  
â”œâ”€â”€ server_cifar100.py  
â”œâ”€â”€ client_svhn.py  
â”œâ”€â”€ server_svhn.py  
â”‚  
â”œâ”€â”€ data/  
â”œâ”€â”€ client_stats_X/  
â””â”€â”€ README.md  

All scripts follow a unified taxonomy:

GH-OFL | DATASET | ROLE | SPACE  

Example:  
GH-OFL | CIFAR-100 | SERVER | x-space  

---

# âš™ï¸ Requirements

- Python â‰¥ 3.9  
- PyTorch â‰¥ 2.0  
- Torchvision â‰¥ 0.15  
- CUDA optional (recommended for QDA)

Install dependencies:

pip install torch torchvision numpy  

---

# ğŸš€ How to Run the Code (Step-by-Step)

## STEP 1 â€” Generate Client Statistics

Run the client script for the desired dataset.

Example (CIFAR-100):

python client_cifar100.py  

What happens:

1. Dataset is downloaded automatically (if not present).
2. Dirichlet split is generated (Î± defined inside the script).
3. ResNet-18 extracts 512-dimensional features.
4. Each client accumulates statistics in float64.
5. Client payloads are saved to:

./client_stats_X/CIFAR100/resnet18-IMAGENET1K_V1_TRAIN_A{alpha}_X512/

Repeat for other datasets:

python client_cifar10.py  
python client_svhn.py  

You only need to generate statistics once per Î± configuration.

---

## STEP 2 â€” Run Server Evaluation

Open the corresponding server script and verify:

STATS_ROOT = "./client_stats_X/..."

Make sure it matches the directory generated in Step 1.

Then run:

python server_cifar100.py  

What the server does:

1. Loads all client .pt files.
2. Aggregates global statistics.
3. Computes:
   - GH-NBdiag
   - GH-LDA
   - GH-QDAfull (if S_per_class_x is available)
4. Builds the Fisher subspace.
5. Synthesizes Fisher-space samples.
6. Trains FisherMix and Proto-Hyper.
7. Evaluates on the test set.

---

## Switching Dirichlet Î±

Inside the server script:

STATS_ROOT = "./client_stats_X/CIFAR100/...A0p1_X512"

Change to:

...A0p5_X512

No other modifications required.

---

# ğŸ–¥ GPU vs CPU Behavior

- Closed-form NB and LDA run on CPU (float64 for stability).
- QDAfull runs on GPU (float32, Cholesky-based, chunked).
- FisherMix and Proto-Hyper train on GPU if available.
- Full CPU execution is supported (slower but correct).

---

# ğŸ”„ Code Update Notice (Important)

This repository corresponds to the final refined camera-ready implementation.

The code has been:

- Refactored for strict numerical stability (explicit float64 accumulation)
- Unified across CIFAR-10, CIFAR-100, and SVHN
- GPU-optimized for full QDA
- Deterministically seeded
- Cleaned for artifact reproducibility
- Explicitly symmetrized covariance matrices
- Shrinkage handling made consistent across heads

---

## âš  Result Differences vs Paper

Due to:

- Improved shrinkage stabilization
- Explicit covariance symmetrization
- Deterministic seed control
- Fisher-space numerical conditioning
- Minor hyperparameter normalization refinements
- Hardware-dependent floating point behavior

Results produced by this repository may differ slightly from those reported in the paper tables.

Differences are typically small and stem from stability and reproducibility improvements.

The implementation remains fully consistent with the theoretical formulation described in the paper.

---

# ğŸ”’ Design Principles

- Statistics-only federation  
- One-shot communication  
- Closed-form analytical heads  
- Controlled Fisher refinement  
- Explicit dtype/device separation  
- Deterministic and reproducible  
- Artifact-review ready  

---

# ğŸ“– Citation

If you use this code, please cite:

@inproceedings{turazza2026ghofl,  
  title={The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics},  
  author={Turazza, Fabio and Picone, Marco and Mamei, Marco},  
  booktitle={International Conference on Learning Representations (ICLR)},  
  year={2026}  
}

---

# ğŸ› Affiliations

Department of Sciences and Methods for Engineering (DISMI)  
Artificial Intelligence Research and Innovation Center (AIRI)  
University of Modena and Reggio Emilia, Italy  

---

## GH-OFL  
### Rethinking Federation Beyond Gradient Aggregation
