<p align="center">
  <img src="assets/GH_OFL_logo.png" width="300">
</p>

# GH-OFL â€” The Gaussian Head Family  
### One-Shot Federated Learning from Client Global Statistics  
**ICLR 2026 â€” Official Camera-Ready Implementation**

---

## ğŸ“Œ Overview

**GH-OFL (Gaussian-Head One-Shot Federated Learning)** is a statistics-driven federated learning framework where clients transmit global feature statistics only, enabling server-side closed-form and lightweight trainable classifiers â€” without gradient exchange, multi-round optimization, or raw data transmission.

This repository contains the refined camera-ready implementation associated with:

Turazza, Picone, Mamei.  
*The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics.*  
ICLR 2026.

ğŸ“„ Paper: https://arxiv.org/abs/2602.01186

---

# ğŸ§  Core Idea

Instead of sharing gradients or local model weights, each client computes and transmits:

- Per-class feature sums (A)
- Per-class diagonal second moments (SUMSQ)
- Full per-class covariance accumulators (S) â€” optional
- Global second-order matrix (B = Î£ xáµ€x)

The server reconstructs Gaussian decision heads analytically and optionally refines them in a Fisher subspace.

âœ” One-shot communication  
âœ” Statistics-only federation  
âœ” No gradient aggregation  
âœ” No raw data exchange  
âœ” No iterative client training  

---

# ğŸ“Š Implemented Heads

## Closed-Form (x-space)

- **GH-NBdiag** â€” Diagonal Gaussian classifier  
- **GH-LDA** â€” Pooled covariance (shrinkage)  
- **GH-QDAfull** â€” Full class covariance (GPU optimized)

## Trainable (Fisher space)

- **FisherMix** â€” Cosine classifier on Fisher projections  
- **Proto-Hyper** â€” Low-rank residual adapter with knowledge distillation  

Proto-Hyper:

Student(z_f) = Standardize(LDA_f(z_f)) + Residual(z_f)  
Teacher = Î»Â·QDA_f + (1âˆ’Î»)Â·LDA_f  
Loss = KL + CE  

Inference uses only the student.

---

# ğŸ“ Repository Structure

```
GH-OFL/
â”‚
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ run_client.py
â”‚   â””â”€â”€ run_client_c100c.py
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ run_server.py
â”‚   â””â”€â”€ run_server_c100c.py
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ cifar10.yaml
â”‚   â”œâ”€â”€ cifar100.yaml
â”‚   â”œâ”€â”€ svhn.yaml
â”‚   â””â”€â”€ cifar100c.yaml
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ client_stats_X/
â”œâ”€â”€ assets/
â””â”€â”€ README.md
```

---

# âš™ï¸ Requirements

- Python â‰¥ 3.9  
- PyTorch â‰¥ 2.0  
- Torchvision â‰¥ 0.15  
- CUDA optional (recommended)

Install:

```bash
pip install torch torchvision numpy pyyaml
```

---

# ğŸš€ How to Run

---

## STEP 1 â€” Generate Client Statistics

### CIFAR-10
```bash
python client/run_client.py --config configs/cifar10.yaml
```

### CIFAR-100
```bash
python client/run_client.py --config configs/cifar100.yaml
```

### SVHN
```bash
python client/run_client.py --config configs/svhn.yaml
```

### CIFAR-100-C (robustness training split)
```bash
python client/run_client_c100c.py --config configs/cifar100c.yaml
```

Client behavior:

1. Downloads dataset automatically (if missing).
2. Applies Dirichlet split (Î± in YAML).
3. Extracts 512-dim features via ResNet-18 (ImageNet).
4. Accumulates float64 statistics.
5. Saves client payloads to:

```
client_stats_X/{DATASET}/resnet18-IMAGENET1K_V1_TRAIN_A{alpha}_X512/
```

---

## STEP 2 â€” Run Server Evaluation

### CIFAR-10
```bash
python server/run_server.py --config configs/cifar10.yaml
```

### CIFAR-100
```bash
python server/run_server.py --config configs/cifar100.yaml
```

### SVHN
```bash
python server/run_server.py --config configs/svhn.yaml
```

### CIFAR-100-C (robustness evaluation)
```bash
python server/run_server_c100c.py --config configs/cifar100c.yaml
```

Server performs:

1. Aggregation of client statistics
2. GH-NBdiag
3. GH-LDA
4. GH-QDAfull (if S exists)
5. Fisher subspace construction
6. Gaussian synthesis
7. FisherMix training
8. Proto-Hyper training
9. Streaming evaluation

---

# ğŸ§ª CIFAR-100-C Robustness

The CIFAR-100-C scripts use the official Zenodo release.

On first execution, the dataset is automatically downloaded (~1.3GB) and extracted to:

```
data/CIFAR-100-C/
```

Default protocol (paper-standard):

- All 19 corruptions
- Severity 5
- 190,000 evaluation images

This is significantly heavier than CIFAR-100 clean (10,000 images).

---

# ğŸ”„ Switching Dirichlet Î±

Edit in YAML:

```yaml
dirichlet_alpha: 0.1
```

Then rerun client + server.

Statistics are stored in separate folders automatically.

---

# ğŸ–¥ Device Behavior

- Aggregation: CPU float64
- QDAfull: GPU float32 (Cholesky)
- Fisher heads: GPU if available
- Fully CPU compatible

---

# âš  Reproducibility Note

Minor deviations from paper tables may occur due to:

- Shrinkage stabilization improvements
- Covariance symmetrization
- Deterministic seed control
- Hardware floating-point variation

The implementation remains fully consistent with the theoretical formulation.

---

# ğŸ“– Citation

```bibtex
@inproceedings{turazza2026ghofl,
  title={The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics},
  author={Turazza Fabio, Picone Marco and Mamei Marco},
  booktitle={ICLR},
  year={2026}
}
```

---

Department of Sciences and Methods for Engineering (DISMI)  
Artificial Intelligence Research and Innovation Center (AIRI)  
University of Modena and Reggio Emilia, Italy  

---

## GH-OFL  
### Rethinking Federation Beyond Gradient Aggregation
